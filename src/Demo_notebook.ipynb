{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_LG-BF9RJWTJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PatchNet**\n",
        "\n",
        "***Authors: Krisztián Bokor, Ginés Carreto Picón, Marc Johler***\n",
        "\n",
        "PatchNet describes a deep learning technique for processing visual information consisting in the split of an image into patches, in order to solve different tasks.\n",
        "\n",
        "This notebook implements the PatchNet idea from the article _[Patch-based reconstruction of a textureless deformable 3d surface from a single rgb image](https://ieeexplore.ieee.org/document/9022546)_, so called _pnBaseline_ model for the purposes of this notebook, in order to predict a depth and normal map given an RGB image.\n",
        "\n",
        "Aditionally, two modifications from the baseline model have been developed, as implementations of the UNet idea (adding multiple connections after each upsampling step in each of the decoders), and a modification of the encoder by using Inception modules (see the report for more info), under the names _pnUNet_ and _pnInception_."
      ],
      "metadata": {
        "id": "IOu20gaVp8a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment & Data\n",
        "\n",
        "This notebook can be executed in a local environment and in Google Colab. In order to train and evaluate the models, we need the dataset for it, so please, follow these instructions to do it depending if you are running this locally or in Google Colab:\n",
        "\n",
        "### Google Colab\n",
        "\n",
        "1. Download the preprocessed dataset [here](https://drive.google.com/file/d/1Wg2dB8y98aektVxC70ZPl62QjtSBxiYZ/view?usp=sharing)\n",
        "2. Upload the downloaded file into your Google Colab, in a folder called `datasets`. Please, mind the compressed space (_3.7 GB_)\n",
        "\n",
        "### Local environment\n",
        "\n",
        "In order to access the library developed for the project, you can download directly the scripts from the [Github repository](https://github.com/Atamarado/DLVR_3DReconstruction). You can find this very same file in the `/src/` folder.\n",
        "\n",
        "1. Download the preprocessed dataset [here](https://drive.google.com/file/d/1Wg2dB8y98aektVxC70ZPl62QjtSBxiYZ/view?usp=sharing)\n",
        "2. Uncompress the downloaded zip into your project folder (where you set up the Github repository under `/data/` folder.\n",
        "\n",
        "\n",
        "Please, keep in mind that the *recommended* version of the `tensorflow` package is 2.9.2 in order to run this notebook with no problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "89YJu_n4t8mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path and preparation code\n",
        "\n"
      ],
      "metadata": {
        "id": "1zQ6Xbm3Rxm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.9.2 in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (2.9.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (0.28.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (2.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (21.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.51.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.2) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.15.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.4.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.9.2) (3.0.9)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "Cloning into 'DLVR_3DReconstruction'...\n",
            "remote: Enumerating objects: 970, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 970 (delta 108), reused 102 (delta 54), pack-reused 805\u001b[K\n",
            "Receiving objects: 100% (970/970), 6.41 MiB | 5.48 MiB/s, done.\n",
            "Resolving deltas: 100% (586/586), done.\n",
            "/content/DLVR_3DReconstruction\n",
            "/content/DLVR_3DReconstruction/src\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive and make a directory\n",
        "\n",
        "    # Install proper version of tensorflow\n",
        "    %pip install tensorflow==2.9.2\n",
        "\n",
        "    # Mount Google Drive and make a directory\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    # Download and unzip data\n",
        "    !7z x '/content/drive/MyDrive/datasets/pnData.zip'\n",
        "\n",
        "    # Clone repo from github\n",
        "    username = 'Atamarado'\n",
        "    repository = 'DLVR_3DReconstruction'\n",
        "\n",
        "    !git clone https://github.com/{username}/{repository}\n",
        "\n",
        "    # Pull from the PatchNet branch\n",
        "    %cd {repository}\n",
        "\n",
        "    # Change to the implementation's directory\n",
        "    %cd 'src'\n",
        "\n",
        "    train_path = \"/content/pnData/train\"\n",
        "else:\n",
        "    train_path = \"data/pnData/train\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JK2EN4zf8uj",
        "outputId": "bf67cc1d-d0d8-4f9d-96ab-ea85b7dfd848"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Used libraries\n",
        "\n",
        "If you want to change the model used, you can switch the first import from `patch.nets.pnBaseline` to `patch.nets.pnUNet` or `patch.nets.pnInception`"
      ],
      "metadata": {
        "id": "krLpQUvFSl5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from patch.nets.pnBaseline import TfNetwork # Valid imports: patch.nets.pnBaseline, patch.nets.pnUNet, patch.nets.pnInception\n",
        "from patch.PatchNet_tf import PatchNet\n",
        "from DataGenerator import DataGenerator\n",
        "from Feed_data import train, test, patch_loop_separate_loss, image_loop, patch_loop\n",
        "\n",
        "%cd .."
      ],
      "metadata": {
        "id": "KbAnlMJQV8ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Here you can find the code to make a full training iteration loop over the selected patchnet type above:\n",
        "\n",
        "## Settings\n",
        "\n",
        "Customizable parameters:"
      ],
      "metadata": {
        "id": "TyHepYpUSoS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "patch_size = 128\n",
        "min_channels = 8\n",
        "batch_size = 32\n",
        "n_val_batches = 20\n",
        "fixed_overlaps = False\n",
        "save_epoch_weigts = False"
      ],
      "metadata": {
        "id": "gusDn-4YWCNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network & DataGenerator"
      ],
      "metadata": {
        "id": "C0X9fKaGS02y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiation of the patchnet and the Data Generator"
      ],
      "metadata": {
        "id": "Ggo21-ap30Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patchnet = PatchNet(patch_size, min_channels, fixed_overlaps, TfNetwork(patch_size, min_channels))\n",
        "datagen = DataGenerator(train_path, batch_size, patching = True, patch_size = patch_size, fixed_overlaps = fixed_overlaps)"
      ],
      "metadata": {
        "id": "2meExZswS6Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "kFgLKxyPS_fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_depth_loss': [],\n",
        "    'train_normal_loss': [],\n",
        "    'validation_loss_patch': [],\n",
        "    'validation_depth_loss_patch': [],\n",
        "    'validation_normal_loss_patch': [],\n",
        "    'validation_loss_image': []\n",
        "}"
      ],
      "metadata": {
        "id": "J1LXCLrbtThl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "        # Train loop epoch\n",
        "        train_loss, train_depth_loss, train_normal_loss = patch_loop_separate_loss(patchnet, datagen, validation = False, n_batches=datagen.__train_len__())\n",
        "        # Validation loop epoch\n",
        "        val_loss_patch, val_depth_loss_patch, val_normal_loss_patch = patch_loop_separate_loss(patchnet, datagen, validation = True, n_batches = datagen.__val_len__())\n",
        "\n",
        "        assert abs(train_loss - (train_depth_loss + train_normal_loss)) < 0.01\n",
        "        assert abs(val_loss_patch - (val_depth_loss_patch + val_normal_loss_patch)) < 0.01\n",
        "\n",
        "        print(\"Epoch\", epoch, \"done with losses:\")\n",
        "        print(\"Training:\", train_loss)\n",
        "        print(\"Training depth:\", train_depth_loss)\n",
        "        print(\"Training normal:\", train_normal_loss)\n",
        "        print(\"Validation on patches\", val_loss_patch)\n",
        "        print(\"Validation on patches depth\", val_depth_loss_patch)\n",
        "        print(\"Validation on patches normal\", val_normal_loss_patch)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_depth_loss'].append(train_depth_loss)\n",
        "        history['train_normal_loss'].append(train_normal_loss)\n",
        "        history['validation_loss_patch'].append(val_loss_patch)\n",
        "        history['validation_depth_loss_patch'].append(val_depth_loss_patch)\n",
        "        history['validation_normal_loss_patch'].append(val_normal_loss_patch)\n",
        "\n",
        "        if save_epoch_weigts:\n",
        "          patchnet.network.save_weights(f'/content/drive/MyDrive/PatchNet/weights/network_epoch{epoch}') "
      ],
      "metadata": {
        "id": "tJMr2IqVTHk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b31e2-b82e-4957-cc47-8456f2f66ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress: 100%|██████████| 529/529 [18:37<00:00,  2.11s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:31<00:00,  1.59s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.8524784, shape=(), dtype=float32) tf.Tensor(0.18005352, shape=(), dtype=float32) tf.Tensor(1.6724248, shape=(), dtype=float32)\n",
            "tf.Tensor(1.615984, shape=(), dtype=float32) tf.Tensor(0.085167915, shape=(), dtype=float32) tf.Tensor(1.5308162, shape=(), dtype=float32)\n",
            "Epoch 0 done with losses:\n",
            "Training: tf.Tensor(1.8524784, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.18005352, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.6724248, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.615984, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.085167915, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.5308162, shape=(), dtype=float32)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress: 100%|██████████| 529/529 [18:30<00:00,  2.10s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:25<00:00,  1.55s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.5855054, shape=(), dtype=float32) tf.Tensor(0.08075252, shape=(), dtype=float32) tf.Tensor(1.5047537, shape=(), dtype=float32)\n",
            "tf.Tensor(1.5708774, shape=(), dtype=float32) tf.Tensor(0.07356367, shape=(), dtype=float32) tf.Tensor(1.4973131, shape=(), dtype=float32)\n",
            "Epoch 1 done with losses:\n",
            "Training: tf.Tensor(1.5855054, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.08075252, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.5047537, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.5708774, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.07356367, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.4973131, shape=(), dtype=float32)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:44<00:00,  2.01s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:25<00:00,  1.55s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.5182052, shape=(), dtype=float32) tf.Tensor(0.07397374, shape=(), dtype=float32) tf.Tensor(1.444233, shape=(), dtype=float32)\n",
            "tf.Tensor(1.5214696, shape=(), dtype=float32) tf.Tensor(0.073481485, shape=(), dtype=float32) tf.Tensor(1.4479874, shape=(), dtype=float32)\n",
            "Epoch 2 done with losses:\n",
            "Training: tf.Tensor(1.5182052, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.07397374, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.444233, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.5214696, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.073481485, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.4479874, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:46<00:00,  2.02s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:27<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.4254255, shape=(), dtype=float32) tf.Tensor(0.06747367, shape=(), dtype=float32) tf.Tensor(1.3579521, shape=(), dtype=float32)\n",
            "tf.Tensor(1.427622, shape=(), dtype=float32) tf.Tensor(0.06346709, shape=(), dtype=float32) tf.Tensor(1.3641547, shape=(), dtype=float32)\n",
            "Epoch 3 done with losses:\n",
            "Training: tf.Tensor(1.4254255, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.06747367, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.3579521, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.427622, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.06346709, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.3641547, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:46<00:00,  2.02s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:26<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.3715193, shape=(), dtype=float32) tf.Tensor(0.06221751, shape=(), dtype=float32) tf.Tensor(1.3093021, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3658587, shape=(), dtype=float32) tf.Tensor(0.06007567, shape=(), dtype=float32) tf.Tensor(1.3057827, shape=(), dtype=float32)\n",
            "Epoch 4 done with losses:\n",
            "Training: tf.Tensor(1.3715193, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.06221751, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.3093021, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.3658587, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.06007567, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.3057827, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:38<00:00,  2.00s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:25<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.3321522, shape=(), dtype=float32) tf.Tensor(0.058282204, shape=(), dtype=float32) tf.Tensor(1.27387, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3303556, shape=(), dtype=float32) tf.Tensor(0.05783757, shape=(), dtype=float32) tf.Tensor(1.2725191, shape=(), dtype=float32)\n",
            "Epoch 5 done with losses:\n",
            "Training: tf.Tensor(1.3321522, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.058282204, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.27387, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.3303556, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.05783757, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.2725191, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:33<00:00,  1.99s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:23<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.3028089, shape=(), dtype=float32) tf.Tensor(0.055723585, shape=(), dtype=float32) tf.Tensor(1.247085, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3202013, shape=(), dtype=float32) tf.Tensor(0.055585165, shape=(), dtype=float32) tf.Tensor(1.2646166, shape=(), dtype=float32)\n",
            "Epoch 6 done with losses:\n",
            "Training: tf.Tensor(1.3028089, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.055723585, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.247085, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.3202013, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.055585165, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.2646166, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:34<00:00,  1.99s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:24<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.2788123, shape=(), dtype=float32) tf.Tensor(0.05423371, shape=(), dtype=float32) tf.Tensor(1.2245779, shape=(), dtype=float32)\n",
            "tf.Tensor(1.3039114, shape=(), dtype=float32) tf.Tensor(0.05356792, shape=(), dtype=float32) tf.Tensor(1.2503437, shape=(), dtype=float32)\n",
            "Epoch 7 done with losses:\n",
            "Training: tf.Tensor(1.2788123, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.05423371, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.2245779, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.3039114, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.05356792, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.2503437, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 529/529 [17:34<00:00,  1.99s/it]\n",
            "Validation progress (patches): 100%|██████████| 133/133 [03:24<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.2567292, shape=(), dtype=float32) tf.Tensor(0.052445587, shape=(), dtype=float32) tf.Tensor(1.2042841, shape=(), dtype=float32)\n",
            "tf.Tensor(1.2807528, shape=(), dtype=float32) tf.Tensor(0.053517446, shape=(), dtype=float32) tf.Tensor(1.227235, shape=(), dtype=float32)\n",
            "Epoch 8 done with losses:\n",
            "Training: tf.Tensor(1.2567292, shape=(), dtype=float32)\n",
            "Training depth: tf.Tensor(0.052445587, shape=(), dtype=float32)\n",
            "Training normal: tf.Tensor(1.2042841, shape=(), dtype=float32)\n",
            "Validation on patches tf.Tensor(1.2807528, shape=(), dtype=float32)\n",
            "Validation on patches depth tf.Tensor(0.053517446, shape=(), dtype=float32)\n",
            "Validation on patches normal tf.Tensor(1.227235, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress:  48%|████▊     | 254/529 [08:28<09:08,  1.99s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Once the training has been completed, we can evaluate its performance.\n",
        "\n",
        "The validation scores are available during training. However, we can evaluate the performance of each model over different classes:"
      ],
      "metadata": {
        "id": "MTQD03o6mXsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen.set_validation(True)\n",
        "\n",
        "for cat in datagen.get_object_categories():\n",
        "  val_loss_patch, val_depth_loss_patch, val_normal_loss_patch = patch_loop_separate_loss_category(patchnet, datagen, validation = True, n_batches=200)\n",
        "  print(\"Validation on patches\", val_loss_patch)\n",
        "  print(\"Validation on patches depth\", val_depth_loss_patch)\n",
        "  print(\"Validation on patches normal\", val_normal_loss_patch)\n",
        "\n",
        "\n",
        "datagen.reset_category()"
      ],
      "metadata": {
        "id": "5jFlLr_6mTUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment from here:"
      ],
      "metadata": {
        "id": "RK6GGSEdu0tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/weights.zip /content/weights"
      ],
      "metadata": {
        "id": "nZYCJCGe1tmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "      # train_loss = patch_loop(patchnet, datagen, validation = False, n_batches=80)\n",
        "      train_loss, train_depth_loss, train_normal_loss = patch_loop_separate_loss(patchnet, datagen, validation = False, n_batches=80)\n",
        "      # val_loss_patch, val_depth_loss_patch, val_normal_loss_patch = patch_loop_separate_loss(patchnet, datagen, validation = True, n_batches = n_val_batches)\n",
        "      # val_loss_img = image_loop(patchnet, datagen, n_batches = n_val_batches)\n",
        "      \n",
        "      print(train_loss, train_depth_loss, train_normal_loss)"
      ],
      "metadata": {
        "id": "FGU1A5eU3Z8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Weights"
      ],
      "metadata": {
        "id": "z8qhfGRpx0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/weights.zip' -d '/' "
      ],
      "metadata": {
        "id": "37WX4Bm9yrxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patchnet = PatchNet(patch_size, min_channels, fixed_overlaps, TfNetwork(patch_size, min_channels))\n",
        "datagen = DataGenerator(train_path, batch_size, patching = True, patch_size = patch_size, fixed_overlaps = fixed_overlaps)"
      ],
      "metadata": {
        "id": "HiuBAWrw13dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patchnet.encoder.layers.load_weights('/content/weights/epoch_9_encoder')\n",
        "patchnet.depth_decoder.layers.load_weights('/content/weights/epoch_9_depth')\n",
        "patchnet.normals_decoder.layers.load_weights('/content/weights/epoch_9_normals')"
      ],
      "metadata": {
        "id": "iLvRjWCu2PuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "nVhSaWvTKKsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "8T0yiyUBI4Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "plt.title('Training Losses')\n",
        "plt.plot(history['train_loss'], label='Total Loss')\n",
        "plt.plot(history['train_depth_loss'], label='Depth Loss')\n",
        "plt.plot(history['train_normal_loss'], label='Normal Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.savefig(fname='/content/training_separate_losses.jpg', pad_inches=0.2, bbox_inches='tight')\n",
        "# plt.plot(history['validation_loss_image'])\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "_CvS5z9FKUqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "plt.title('Training Losses')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(history['train_loss'], 'r-', label='Total Loss')\n",
        "ax1.plot(history['train_normal_loss'], 'g-', label='Normal Loss')\n",
        "ax2.plot(history['train_depth_loss'], 'b-', label='Depth Loss')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "fig.legend()\n",
        "# ax2.set_ylabel('Y2 data', color='b')\n",
        "\n",
        "plt.savefig('/content/training_separate_losses.jpg')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "gfZTpEBlF_wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction visualization"
      ],
      "metadata": {
        "id": "_LG-BF9RJWTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from src.patch.Losses import depth_loss"
      ],
      "metadata": {
        "id": "IDVPhpxe28w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = datagen.__getitem__(0)\n",
        "pred_depth, pred_normal = patchnet(x[:,:,:,:3])"
      ],
      "metadata": {
        "id": "AN6fFVKpJTOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(pred_depth.shape)\n",
        "print(pred_normal.shape)\n",
        "\n",
        "patch_shape = (1, 128, 128, 1)\n",
        "pred = pred_normal[0]\n",
        "gt = y[0, :, :, 1:]\n",
        "fg_mask = x[0, :, :, 3]"
      ],
      "metadata": {
        "id": "93f15yDB8w6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(depth_loss(tf.reshape(pred, patch_shape), tf.reshape(gt, patch_shape), tf.reshape(fg_mask, patch_shape)))\n",
        "print(tf.reduce_sum(tf.abs(pred * fg_mask - gt * fg_mask)))\n",
        "print(tf.reduce_sum(fg_mask))\n",
        "print(fg_mask)\n",
        "print(tf.reduce_sum(tf.abs(pred * fg_mask - gt * fg_mask )) / tf.reduce_sum(fg_mask))\n"
      ],
      "metadata": {
        "id": "-D13m-yrGdej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vmax = max(tf.reduce_max(fg_mask * gt), tf.reduce_max(fg_mask * pred))\n",
        "vmin = min(tf.reduce_min(fg_mask * gt), tf.reduce_min(fg_mask * pred))"
      ],
      "metadata": {
        "id": "jukiamox_a0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colors"
      ],
      "metadata": {
        "id": "FKmvHAiWEEuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg_mask_broad = tf.tile(tf.reshape(fg_mask, (128, 128, 1)), [1, 1, 3])\n",
        "plt.imshow(gt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7y0MMtY-8rWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(tf.where(fg_mask == 0, fg_mask, gt), norm=colors.LogNorm())\n",
        "print(tf.where(fg_mask == 0, fg_mask, gt))\n",
        "print(tf.where(fg_mask == 0, fg_mask, pred))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(tf.where(fg_mask == 0, fg_mask, pred), norm=colors.LogNorm())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UDtNxG8-9vEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "FRy_E4ceJ17d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal = np.load('/content/pnData/train/normals/cloth_Lc_left_edge_0052.npz')['normals']\n",
        "normal = normal[:,:,::-1]\n",
        "plt.imshow((normal + 1) / 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Sknrec1J4fB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}